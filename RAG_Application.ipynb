{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WZp8J48mB68"
      },
      "source": [
        "# ðŸ“˜ RAG Application (DMP Example) using LangChain + **Local Llama (Ollama)** + FAISS\n",
        "\n",
        "\n",
        "âœ… We use:\n",
        "- **Ollama** to run a local Llama model for generation (e.g., `llama3.2`, `llama3.1`, etc.)\n",
        "- **Ollama embeddings** for vector search (e.g., `nomic-embed-text` or `mxbai-embed-large`)\n",
        "- **FAISS** as the vector store\n",
        "\n",
        "You will:\n",
        "1. Use couple of PDFs\n",
        "2. Load it\n",
        "3. Chunk it\n",
        "4. Embed + index with FAISS\n",
        "5. Build:\n",
        "   - **Baseline (no RAG)**\n",
        "   - **RAG (retrieve + grounded answer)**\n",
        "6. Compare results for the same DMP question\n",
        "\n",
        "---\n",
        "## ðŸ”´ Important\n",
        "You must have **Ollama installed and running** on your machine.\n",
        "\n",
        "- Install Ollama: https://ollama.com\n",
        "- Then pull models from the terminal (or from notebook cells below):\n",
        "  - `ollama pull llama3.2`\n",
        "  - `ollama pull nomic-embed-text`  *(embedding model)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-fUCj0KmJGX"
      },
      "source": [
        "## ðŸ”§ 1) Install Python dependencies (run once)\n",
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ¦™ 2) Make sure Ollama is installed + pull models\n",
        "\n",
        "Run these in your terminal **once** (recommended):\n",
        "```bash\n",
        "ollama pull llama3.2\n",
        "ollama pull nomic-embed-text\n",
        "```\n",
        "\n",
        "Or you can try running the commands below from the notebook.  \n",
        "(If your environment doesn't allow shell commands, do it in the terminal.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jmGu5Lr-mPZG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Ollama models configured: llama3.2 | nomic-embed-text\n"
          ]
        }
      ],
      "source": [
        "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "\n",
        "# --- Choose models you have pulled via `ollama pull ...`\n",
        "LLM_MODEL = \"llama3.2\"            # generation model\n",
        "EMBED_MODEL = \"nomic-embed-text\"  # embeddings model (recommended)\n",
        "\n",
        "# Initialize Ollama chat model\n",
        "llm = ChatOllama(model=LLM_MODEL, temperature=0.2)\n",
        "\n",
        "# Initialize Ollama embeddings model\n",
        "embeddings = OllamaEmbeddings(model=EMBED_MODEL)\n",
        "\n",
        "print(\"âœ… Ollama models configured:\", LLM_MODEL, \"|\", EMBED_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYPoOcMVutNQ"
      },
      "source": [
        "## ðŸ“„ 3) Load Load the document using LangChain\n",
        "\n",
        "For a blog/tutorial example, we use a couple of NIH DMS plan pdf.\n",
        "In production, you would replace this with:\n",
        "- NIH policy pages (HTML/PDF)\n",
        "- DMPTool guidance text\n",
        "- Institutional sharing policy text\n",
        "- Repository documentation\n",
        "\n",
        "The key idea: **RAG retrieves from these sources** instead of the LLM guessing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 3 PDF(s)\n",
            "Total page-docs: 9\n",
            "Example metadata: {'producer': 'www.ilovepdf.com', 'creator': 'MicrosoftÂ® Word 2016', 'creationdate': '2025-07-08T17:11:53+00:00', 'moddate': '2025-07-08T17:11:53+00:00', 'source': 'data\\\\Human_4.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': 'Human_4.pdf'}\n",
            "Example text (first 400 chars): Data Management and Sharing Plan \n",
            "Element 1: Data Type: \n",
            "A. Types and amount of scientific data expected to be generated in the project:  \n",
            "Summarize the types and estimated amount of scientific data expected to be generated in the project. \n",
            "This project will perform secondary data analysis on kidney magnetic resonance imaging (MRI) data to \n",
            "determine the parenchymal kidney volume. Analysis will be\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "try:\n",
        "    from langchain_ollama import ChatOllama\n",
        "except Exception:\n",
        "    from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "\n",
        "DATA_DIR = Path(\"data\")\n",
        "pdf_paths = sorted(DATA_DIR.glob(\"*.pdf\"))\n",
        "\n",
        "if not pdf_paths:\n",
        "    raise FileNotFoundError(\"No PDFs found in ./data. Put NIH PDFs inside the data folder.\")\n",
        "\n",
        "docs = []\n",
        "for pdf_path in pdf_paths:\n",
        "    loader = PyPDFLoader(str(pdf_path))\n",
        "    pdf_docs = loader.load()  # one Document per page\n",
        "    for d in pdf_docs:\n",
        "        d.metadata[\"source_file\"] = pdf_path.name\n",
        "        # PyPDFLoader usually sets page in metadata; keep it consistent:\n",
        "        d.metadata[\"page\"] = d.metadata.get(\"page\", None)\n",
        "    docs.extend(pdf_docs)\n",
        "\n",
        "print(f\"Loaded {len(pdf_paths)} PDF(s)\")\n",
        "print(f\"Total page-docs: {len(docs)}\")\n",
        "print(\"Example metadata:\", docs[0].metadata)\n",
        "print(\"Example text (first 400 chars):\", docs[0].page_content[:400])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ‚ï¸ 4) Chunk the text\n",
        "\n",
        "Why chunk?\n",
        "- Embeddings work best on moderate-size text segments\n",
        "- Retrieval becomes more precise\n",
        "\n",
        "We use `RecursiveCharacterTextSplitter` to chunk with overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total chunks: 31\n",
            "Sample chunk metadata: {'producer': 'www.ilovepdf.com', 'creator': 'MicrosoftÂ® Word 2016', 'creationdate': '2025-07-08T17:11:53+00:00', 'moddate': '2025-07-08T17:11:53+00:00', 'source': 'data\\\\Human_4.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': 'Human_4.pdf', 'chunk_id': 'chunk_000000'}\n",
            "Sample chunk text (first 300 chars): Data Management and Sharing Plan \n",
            "Element 1: Data Type: \n",
            "A. Types and amount of scientific data expected to be generated in the project:  \n",
            "Summarize the types and estimated amount of scientific data expected to be generated in the project. \n",
            "This project will perform secondary data analysis on kidney\n"
          ]
        }
      ],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=150,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "# Add chunk_id for citations\n",
        "for i, c in enumerate(chunks):\n",
        "    c.metadata[\"chunk_id\"] = f\"chunk_{i:06d}\"\n",
        "\n",
        "print(\"Total chunks:\", len(chunks))\n",
        "print(\"Sample chunk metadata:\", chunks[0].metadata)\n",
        "print(\"Sample chunk text (first 300 chars):\", chunks[0].page_content[:300])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ” 5) Build FAISS vector store \n",
        "\n",
        "- Embed each chunk\n",
        "- Store in FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Nahid\\AppData\\Local\\Temp\\ipykernel_43760\\3264666804.py:4: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaEmbeddings``.\n",
            "  embeddings = OllamaEmbeddings(model=EMBED_MODEL, base_url=BASE_URL)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS index built and saved to ./faiss_index_nih\n"
          ]
        }
      ],
      "source": [
        "EMBED_MODEL = \"nomic-embed-text\"   # or \"mxbai-embed-large\"\n",
        "BASE_URL = \"http://localhost:11434\"\n",
        "\n",
        "embeddings = OllamaEmbeddings(model=EMBED_MODEL, base_url=BASE_URL)\n",
        "\n",
        "# Build FAISS index\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "# Optional: save index locally\n",
        "vectorstore.save_local(\"faiss_index_nih\")\n",
        "print(\"FAISS index built and saved to ./faiss_index_nih\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§  6) Build retriever + helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "def format_docs_for_prompt(docs):\n",
        "    \"\"\"Create a compact context string + show citations info.\"\"\"\n",
        "    blocks = []\n",
        "    for d in docs:\n",
        "        src = d.metadata.get(\"source_file\", \"unknown\")\n",
        "        page = d.metadata.get(\"page\", \"NA\")\n",
        "        cid = d.metadata.get(\"chunk_id\", \"NA\")\n",
        "        text = d.page_content.strip()\n",
        "        blocks.append(f\"[{cid} | {src} | page={page}]\\n{text}\")\n",
        "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
        "\n",
        "def extract_citations(text):\n",
        "    \"\"\"Find chunk citations like chunk_000123 in the answer.\"\"\"\n",
        "    return sorted(set(re.findall(r\"chunk_\\d{6}\", text)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§  7) Build Baseline vs RAG pipelines\n",
        "\n",
        "Weâ€™ll do it in a simple, explicit way (easy to understand for a blog post).\n",
        "\n",
        "### Baseline (no RAG)\n",
        "- Just ask the Llama model directly\n",
        "\n",
        "### RAG\n",
        "- Retrieve top-k relevant chunks\n",
        "- Inject them into the prompt\n",
        "- Ask model to cite chunk IDs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.1 Baseline (NO RAG) answer function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "LLM_MODEL = \"llama3.2\"\n",
        "\n",
        "llm = ChatOllama(model=LLM_MODEL, base_url=BASE_URL, temperature=0.2)\n",
        "\n",
        "def answer_without_rag(question: str) -> str:\n",
        "    prompt = f\"\"\"You are helping write an NIH Data Management and Sharing (DMS) Plan.\n",
        "\n",
        "Answer the question clearly and concisely.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Return:\n",
        "- Answer: (paragraph)\n",
        "- Notes: (bullets, if needed)\n",
        "\"\"\"\n",
        "    return llm.invoke(prompt).content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.2) RAG answer function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def answer_with_rag(question: str, k: int = 5) -> dict:\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
        "\n",
        "    # âœ… New LangChain API\n",
        "    retrieved = retriever.invoke(question)\n",
        "\n",
        "    context = format_docs_for_prompt(retrieved)\n",
        "\n",
        "    prompt = f\"\"\"You are helping write an NIH Data Management and Sharing (DMS) Plan.\n",
        "\n",
        "Use ONLY the context below to answer. If the context is insufficient, say what is missing.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Return exactly:\n",
        "- Answer: (paragraph)\n",
        "- Key points: (bullets)\n",
        "- Citations: (list chunk_ids you relied on, like chunk_000123)\n",
        "\"\"\"\n",
        "    out = llm.invoke(prompt).content\n",
        "    cited = extract_citations(out)\n",
        "\n",
        "    return {\n",
        "        \"answer\": out,\n",
        "        \"retrieved_docs\": retrieved,\n",
        "        \"retrieved_chunk_ids\": [d.metadata.get(\"chunk_id\") for d in retrieved],\n",
        "        \"cited_chunk_ids\": cited\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Quick test with one question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== NO RAG ====\n",
            "Here are the extracted information from the PDF DMS plan example:\n",
            "\n",
            "* Repository name used for deposit: \"National Center for Biotechnology Information (NCBI) Gene Expression Omnibus (GEO)\"\n",
            "* When data will be released: \"Data will be made available to the public 12 months after publication of the final manuscript.\"\n",
            "* How long data remains available: \"Data will remain available on GEO for a minimum of 10 years from the date of deposit, and will be periodically reviewed and updated as necessary.\"\n",
            "* Access model used: \"Tiered access\"\n",
            "* Agreement/contract required: \"Researchers must agree to comply with the terms of the NIH Data Sharing Policy and the NCBI Data Sharing Agreement.\"\n",
            "\n",
            "Note: The exact wording may vary depending on the specific DMS plan example, but these are the extracted information based on a typical example.\n",
            "\n",
            "==== WITH RAG ====\n",
            "Here are the extracted answers:\n",
            "\n",
            "- Answer: \n",
            "The data will be made available through the dbGaP following standard review and release protocols. The data will be released by the time of publication or the end of the project period, whichever comes first.\n",
            "\n",
            "- Key points:\n",
            "â€¢ Repository name used for deposit: dbGaP\n",
            "â€¢ When data will be released (timing language): \"whichever comes first\"\n",
            "â€¢ How long data remains available: Indefinitely through the repository websites\n",
            "â€¢ Access model is not explicitly stated, but it implies that all data will be made available to researchers upon release.\n",
            "\n",
            "- Citations:\n",
            "â€¢ chunk_000015 | Human_6.pdf | page=1\n",
            "â€¢ chunk_000027 | Human_8.pdf | page=2\n",
            "\n",
            "---- RAG Evidence (Retrieved Chunks) ----\n",
            "\n",
            "[chunk_000005 | Human_4.pdf | page=1]\n",
            "Describe when the scientific data will be made available to other users (i.e., no later than the time of an \n",
            "associated publication or end of the performance period, whichever comes first) and for how long data will be \n",
            "available.  \n",
            "The data will be made available through the dbGaP following the standard review and release protocols. \n",
            "The data will be made available as soon as possible. Scientific data included in published manuscripts will \n",
            "be made available at the time of publication; all other scientific data will be made available no later than \n",
            "the end of the award. \n",
            "Element 5: Access, Distribution, or Reuse Considerations:  \n",
            "A. Factors affecting subsequent access, distribution, or reus\n",
            "\n",
            "[chunk_000025 | Human_8.pdf | page=1]\n",
            "rigor and reproducibility. Particularly, the preclinical efficacy studies will follow the general ARRIVE \n",
            "guidelines for animal research and the best practice guidelines for AD preclinical efficacy studies. \n",
            " \n",
            "Element 4: Data Preservation, Access, and Associated Timelines: \n",
            "A. Repository where scientific data and metadata will be archived:  \n",
            "Provide the name of the repository(ies) where scientific data and met adata arising from the project will be \n",
            "archived. \n",
            "Compound analogs design, production, characterization, and purification protocols will be deposited in \n",
            "PubChem (analogs and physicochemical characteristics) and at the AD Knowledge Portal. Data generated \n",
            "from Compound A analogs in vi\n",
            "\n",
            "[chunk_000003 | Human_4.pdf | page=0]\n",
            "interoperability of datasets and resources and provide the name(s) of the data standards that will be applied \n",
            "and describe how these data standards will be applied to the scientific data generated by the research \n",
            "proposed in this project. If applicable, indicate that no consensus standards exist. \n",
            "The data that will be used for the proposed secondary data analysis is already available on the dbGaP. The \n",
            "documentation for the generation of the data is also available. The estimated kidney length and volumes \n",
            "will be reported in the community standard of centimeters and milliliters. \n",
            "Element 4: Data Preservation, Access, and Associated Timelines:\n",
            "\n",
            "[chunk_000027 | Human_8.pdf | page=2]\n",
            "The research community will have access to data at publication or at the end of the award, wh ichever \n",
            "comes first. AD Knowledge portal standard submission deadlines will be taken into consideration to \n",
            "comply with the DMS timeline requirements. Studies will be uploaded to the AD Knowledge Portal prior to \n",
            "publication to include their own digital obje ct identifiers (DOI) to aid in findability. We will include that \n",
            "DOI in the relevant publications. The AD Knowledge Portal will make decisions about how long to preserve \n",
            "the data. This repository has not deleted any deposited data as far as we know. \n",
            "Element 5: Access, Distribution, or Reuse Considerations:  \n",
            "A. Factors affecting subsequent a\n",
            "\n",
            "[chunk_000015 | Human_6.pdf | page=1]\n",
            "Here]and other search engines. \n",
            " \n",
            "C. When and how long the scientific data will be made available:  \n",
            "Describe when the scientific data will be made available to other users (i.e., no later than the time of an \n",
            "associated publication or end of the performance period, whichever comes first) and for how long data will be \n",
            "available.  \n",
            "All data will be released by the time of publication or the end of the project period, whichever comes first, \n",
            "and will be available indefinitely through the repository websites. \n",
            "Element 5: Access, Distribution, or Reuse Considerations:  \n",
            "A. Factors affecting subsequent access, distribution, or reuse of scientific data:  \n",
            "NIH expects that in drafting Plans, resea\n",
            "\n",
            "Retrieved chunk ids: ['chunk_000005', 'chunk_000025', 'chunk_000003', 'chunk_000027', 'chunk_000015']\n",
            "Cited chunk ids: ['chunk_000015', 'chunk_000027']\n"
          ]
        }
      ],
      "source": [
        "question = (\n",
        "    \"From the PDF DMS plan example, extract the EXACT: \"\n",
        "    \"(1) repository name used for deposit, \"\n",
        "    \"(2) when data will be released (timing language), \"\n",
        "    \"(3) how long data remains available, \"\n",
        "    \"(4) what access model is used (e.g., tiered access) and what agreement/contract is required. \"\n",
        "    \"Return bullets and include short quotes.\"\n",
        ")\n",
        "\n",
        "print(\"==== NO RAG ====\")\n",
        "print(answer_without_rag(question))\n",
        "\n",
        "print(\"\\n==== WITH RAG ====\")\n",
        "rag = answer_with_rag(question, k=5)\n",
        "print(rag[\"answer\"])\n",
        "\n",
        "print(\"\\n---- RAG Evidence (Retrieved Chunks) ----\")\n",
        "for d in rag[\"retrieved_docs\"]:\n",
        "    print(f\"\\n[{d.metadata.get('chunk_id')} | {d.metadata.get('source_file')} | page={d.metadata.get('page')}]\")\n",
        "    print(d.page_content[:700])\n",
        "\n",
        "print(\"\\nRetrieved chunk ids:\", rag[\"retrieved_chunk_ids\"])\n",
        "print(\"Cited chunk ids:\", rag[\"cited_chunk_ids\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv (3.10.11)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
