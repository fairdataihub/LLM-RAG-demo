{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WZp8J48mB68"
      },
      "source": [
        "# ðŸ“˜ RAG Application (DMP Example) using LangChain + **Local Llama (Ollama)** + FAISS\n",
        "\n",
        "\n",
        "âœ… We use:\n",
        "- **Ollama** to run a local Llama model for generation (e.g., `llama3.2`, `llama3.1`, etc.)\n",
        "- **Ollama embeddings** for vector search (e.g., `nomic-embed-text` or `mxbai-embed-large`)\n",
        "- **FAISS** as the vector store\n",
        "\n",
        "You will:\n",
        "1. Create a small DMP guidance file (`dmp_guidance.txt`)\n",
        "2. Load it\n",
        "3. Chunk it\n",
        "4. Embed + index with FAISS\n",
        "5. Build:\n",
        "   - **Baseline (no RAG)**\n",
        "   - **RAG (retrieve + grounded answer)**\n",
        "6. Compare results for the same DMP question\n",
        "\n",
        "---\n",
        "## ðŸ”´ Important\n",
        "You must have **Ollama installed and running** on your machine.\n",
        "\n",
        "- Install Ollama: https://ollama.com\n",
        "- Then pull models from the terminal (or from notebook cells below):\n",
        "  - `ollama pull llama3.2`\n",
        "  - `ollama pull nomic-embed-text`  *(embedding model)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-fUCj0KmJGX"
      },
      "source": [
        "## ðŸ”§ 1) Install Python dependencies (run once)\n",
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ¦™ 2) Make sure Ollama is installed + pull models\n",
        "\n",
        "Run these in your terminal **once** (recommended):\n",
        "```bash\n",
        "ollama pull llama3.2\n",
        "ollama pull nomic-embed-text\n",
        "```\n",
        "\n",
        "Or you can try running the commands below from the notebook.  \n",
        "(If your environment doesn't allow shell commands, do it in the terminal.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jmGu5Lr-mPZG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Ollama models configured: llama3.2 | nomic-embed-text\n"
          ]
        }
      ],
      "source": [
        "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "\n",
        "# --- Choose models you have pulled via `ollama pull ...`\n",
        "LLM_MODEL = \"llama3.2\"            # generation model\n",
        "EMBED_MODEL = \"nomic-embed-text\"  # embeddings model (recommended)\n",
        "\n",
        "# Initialize Ollama chat model\n",
        "llm = ChatOllama(model=LLM_MODEL, temperature=0.2)\n",
        "\n",
        "# Initialize Ollama embeddings model\n",
        "embeddings = OllamaEmbeddings(model=EMBED_MODEL)\n",
        "\n",
        "print(\"âœ… Ollama models configured:\", LLM_MODEL, \"|\", EMBED_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYPoOcMVutNQ"
      },
      "source": [
        "## ðŸ“„ 3) Load Load the document using LangChain\n",
        "\n",
        "For a blog/tutorial example, we use a couple of NIH DMS plan pdf.\n",
        "In production, you would replace this with:\n",
        "- NIH policy pages (HTML/PDF)\n",
        "- DMPTool guidance text\n",
        "- Institutional sharing policy text\n",
        "- Repository documentation\n",
        "\n",
        "The key idea: **RAG retrieves from these sources** instead of the LLM guessing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 3 PDF(s)\n",
            "Total page-docs: 9\n",
            "Example metadata: {'producer': 'www.ilovepdf.com', 'creator': 'MicrosoftÂ® Word 2016', 'creationdate': '2025-07-08T17:11:53+00:00', 'moddate': '2025-07-08T17:11:53+00:00', 'source': 'data\\\\Human_4.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': 'Human_4.pdf'}\n",
            "Example text (first 400 chars): Data Management and Sharing Plan \n",
            "Element 1: Data Type: \n",
            "A. Types and amount of scientific data expected to be generated in the project:  \n",
            "Summarize the types and estimated amount of scientific data expected to be generated in the project. \n",
            "This project will perform secondary data analysis on kidney magnetic resonance imaging (MRI) data to \n",
            "determine the parenchymal kidney volume. Analysis will be\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# PDF loader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Chunking\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Vector store\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Embeddings (Ollama)\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "\n",
        "# LLM (Ollama) - try new package first, then fallback\n",
        "try:\n",
        "    from langchain_ollama import ChatOllama\n",
        "except Exception:\n",
        "    from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "\n",
        "DATA_DIR = Path(\"data\")\n",
        "pdf_paths = sorted(DATA_DIR.glob(\"*.pdf\"))\n",
        "\n",
        "if not pdf_paths:\n",
        "    raise FileNotFoundError(\"No PDFs found in ./data. Put NIH PDFs inside the data folder.\")\n",
        "\n",
        "docs = []\n",
        "for pdf_path in pdf_paths:\n",
        "    loader = PyPDFLoader(str(pdf_path))\n",
        "    pdf_docs = loader.load()  # one Document per page\n",
        "    for d in pdf_docs:\n",
        "        d.metadata[\"source_file\"] = pdf_path.name\n",
        "        # PyPDFLoader usually sets page in metadata; keep it consistent:\n",
        "        d.metadata[\"page\"] = d.metadata.get(\"page\", None)\n",
        "    docs.extend(pdf_docs)\n",
        "\n",
        "print(f\"Loaded {len(pdf_paths)} PDF(s)\")\n",
        "print(f\"Total page-docs: {len(docs)}\")\n",
        "print(\"Example metadata:\", docs[0].metadata)\n",
        "print(\"Example text (first 400 chars):\", docs[0].page_content[:400])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ‚ï¸ 4) Chunk the text\n",
        "\n",
        "Why chunk?\n",
        "- Embeddings work best on moderate-size text segments\n",
        "- Retrieval becomes more precise\n",
        "\n",
        "We use `RecursiveCharacterTextSplitter` to chunk with overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total chunks: 31\n",
            "Sample chunk metadata: {'producer': 'www.ilovepdf.com', 'creator': 'MicrosoftÂ® Word 2016', 'creationdate': '2025-07-08T17:11:53+00:00', 'moddate': '2025-07-08T17:11:53+00:00', 'source': 'data\\\\Human_4.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': 'Human_4.pdf', 'chunk_id': 'chunk_000000'}\n",
            "Sample chunk text (first 300 chars): Data Management and Sharing Plan \n",
            "Element 1: Data Type: \n",
            "A. Types and amount of scientific data expected to be generated in the project:  \n",
            "Summarize the types and estimated amount of scientific data expected to be generated in the project. \n",
            "This project will perform secondary data analysis on kidney\n"
          ]
        }
      ],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=150,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "# Add chunk_id for citations\n",
        "for i, c in enumerate(chunks):\n",
        "    c.metadata[\"chunk_id\"] = f\"chunk_{i:06d}\"\n",
        "\n",
        "print(\"Total chunks:\", len(chunks))\n",
        "print(\"Sample chunk metadata:\", chunks[0].metadata)\n",
        "print(\"Sample chunk text (first 300 chars):\", chunks[0].page_content[:300])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ” 5) Build FAISS vector store + retriever (with Ollama embeddings)\n",
        "\n",
        "- Embed each chunk\n",
        "- Store in FAISS\n",
        "- Create retriever "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Nahid\\AppData\\Local\\Temp\\ipykernel_43760\\3264666804.py:4: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaEmbeddings``.\n",
            "  embeddings = OllamaEmbeddings(model=EMBED_MODEL, base_url=BASE_URL)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS index built and saved to ./faiss_index_nih\n"
          ]
        }
      ],
      "source": [
        "EMBED_MODEL = \"nomic-embed-text\"   # or \"mxbai-embed-large\"\n",
        "BASE_URL = \"http://localhost:11434\"\n",
        "\n",
        "embeddings = OllamaEmbeddings(model=EMBED_MODEL, base_url=BASE_URL)\n",
        "\n",
        "# Build FAISS index\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "# Optional: save index locally\n",
        "vectorstore.save_local(\"faiss_index_nih\")\n",
        "print(\"FAISS index built and saved to ./faiss_index_nih\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§  6) Build retriever + helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "def format_docs_for_prompt(docs):\n",
        "    \"\"\"Create a compact context string + show citations info.\"\"\"\n",
        "    blocks = []\n",
        "    for d in docs:\n",
        "        src = d.metadata.get(\"source_file\", \"unknown\")\n",
        "        page = d.metadata.get(\"page\", \"NA\")\n",
        "        cid = d.metadata.get(\"chunk_id\", \"NA\")\n",
        "        text = d.page_content.strip()\n",
        "        blocks.append(f\"[{cid} | {src} | page={page}]\\n{text}\")\n",
        "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
        "\n",
        "def extract_citations(text):\n",
        "    \"\"\"Find chunk citations like chunk_000123 in the answer.\"\"\"\n",
        "    return sorted(set(re.findall(r\"chunk_\\d{6}\", text)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§  7) Build Baseline vs RAG pipelines\n",
        "\n",
        "Weâ€™ll do it in a simple, explicit way (easy to understand for a blog post).\n",
        "\n",
        "### Baseline (no RAG)\n",
        "- Just ask the Llama model directly\n",
        "\n",
        "### RAG\n",
        "- Retrieve top-k relevant chunks\n",
        "- Inject them into the prompt\n",
        "- Ask model to cite chunk IDs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.1 Baseline (NO RAG) answer function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "LLM_MODEL = \"llama3.2\"\n",
        "\n",
        "llm = ChatOllama(model=LLM_MODEL, base_url=BASE_URL, temperature=0.2)\n",
        "\n",
        "def answer_without_rag(question: str) -> str:\n",
        "    prompt = f\"\"\"You are helping write an NIH Data Management and Sharing (DMS) Plan.\n",
        "\n",
        "Answer the question clearly and concisely.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Return:\n",
        "- Answer: (paragraph)\n",
        "- Notes: (bullets, if needed)\n",
        "\"\"\"\n",
        "    return llm.invoke(prompt).content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.2 RAG answer function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def answer_with_rag(question: str, k: int = 5) -> dict:\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
        "\n",
        "    # âœ… New LangChain API\n",
        "    retrieved = retriever.invoke(question)\n",
        "\n",
        "    context = format_docs_for_prompt(retrieved)\n",
        "\n",
        "    prompt = f\"\"\"You are helping write an NIH Data Management and Sharing (DMS) Plan.\n",
        "\n",
        "Use ONLY the context below to answer. If the context is insufficient, say what is missing.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Return exactly:\n",
        "- Answer: (paragraph)\n",
        "- Key points: (bullets)\n",
        "- Citations: (list chunk_ids you relied on, like chunk_000123)\n",
        "\"\"\"\n",
        "    out = llm.invoke(prompt).content\n",
        "    cited = extract_citations(out)\n",
        "\n",
        "    return {\n",
        "        \"answer\": out,\n",
        "        \"retrieved_docs\": retrieved,\n",
        "        \"retrieved_chunk_ids\": [d.metadata.get(\"chunk_id\") for d in retrieved],\n",
        "        \"cited_chunk_ids\": cited\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Quick test with one question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== NO RAG ====\n",
            "Data sharing under the NIH Data Management and Sharing (DMS) policy should occur in a manner that balances the need to share research findings with the need to protect sensitive information. The NIH recommends sharing data as soon as possible after publication or when it is no longer necessary for the conduct of the research, but not so early that it compromises the integrity of the data.\n",
            "\n",
            "For sensitive human data, access controls should be implemented to ensure that only authorized individuals have access to the data. Acceptable access controls include:\n",
            "\n",
            "* De-identification: removing personally identifiable information (PII) from the data\n",
            "* Anonymization: using techniques such as pseudonymization or encryption to protect individual identities\n",
            "* Data encryption: protecting data with strong encryption methods, such as SSL/TLS\n",
            "* Access controls: implementing user authentication and authorization mechanisms to control who can access the data\n",
            "* Data sharing agreements: entering into agreements with collaborators or other organizations that outline terms for data sharing and access\n",
            "\n",
            "It is also recommended to use secure data storage and transmission methods, such as encrypted files and secure online repositories, to protect sensitive human data.\n",
            "\n",
            "==== WITH RAG ====\n",
            "Answer:\n",
            "\n",
            "Data will be made available to other users in accordance with the NIH Data Management and Sharing (DMS) policy. According to the policy, data will be shared no later than the time of an associated publication or end of the performance period, whichever comes first. The data will also be made available as soon as possible.\n",
            "\n",
            "Access controls for sensitive human data will be implemented through a tiered system based on data sensitivity. All tiers of data require a data-use agreement at a minimum, with additional precautions for restricted-use and high-restricted-use data. For virtual enclave data, a separate application process will be required to access the data. Researchers who wish to use the most sensitive data must apply for access through this process.\n",
            "\n",
            "Key points:\n",
            "\n",
            "* Data will be made available no later than the time of an associated publication or end of the performance period.\n",
            "* Access controls will be implemented through a tiered system based on data sensitivity.\n",
            "* A data-use agreement is required for all tiers of data, with additional precautions for restricted-use and high-restricted-use data.\n",
            "* Virtual enclave data requires a separate application process to access.\n",
            "* Researchers must apply for access to the most sensitive data.\n",
            "\n",
            "Citations:\n",
            "\n",
            "* chunk_000005 | Human_4.pdf | page=1\n",
            "* chunk_000006 | Human_4.pdf | page=1\n",
            "* chunk_000007 | Human_4.pdf | page=1\n",
            "\n",
            "Retrieved chunk ids: ['chunk_000018', 'chunk_000016', 'chunk_000005', 'chunk_000006', 'chunk_000007']\n",
            "Cited chunk ids: ['chunk_000005', 'chunk_000006', 'chunk_000007']\n"
          ]
        }
      ],
      "source": [
        "question = \"When should data be shared under the NIH DMS policy, and what access controls are acceptable for sensitive human data?\"\n",
        "\n",
        "print(\"==== NO RAG ====\")\n",
        "print(answer_without_rag(question))\n",
        "\n",
        "print(\"\\n==== WITH RAG ====\")\n",
        "rag = answer_with_rag(question, k=5)\n",
        "print(rag[\"answer\"])\n",
        "print(\"\\nRetrieved chunk ids:\", rag[\"retrieved_chunk_ids\"])\n",
        "print(\"Cited chunk ids:\", rag[\"cited_chunk_ids\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv (3.10.11)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
