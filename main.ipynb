{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§­ Project Purpose\n",
        "\n",
        "This mini-project demonstrates **Retrieval-Augmented Generation (RAG)** versus **No-RAG** using content collected from the **FAIR Data Innovations Hub** website.\n",
        "\n",
        "### What we will build:\n",
        "\n",
        "- A simple pipeline that **downloads ~20 web pages** from `fairdataihub.org` and saves them as plain text.\n",
        "- A **vector index (FAISS)** built from those texts using **Ollama embeddings**.\n",
        "- Two answering modes for the **same questions**:\n",
        "  - **No-RAG:** the LLM answers using only its general knowledge.\n",
        "  - **RAG:** the LLM answers using **retrieved website context**, improving grounding and reducing hallucinations.\n",
        "\n",
        "### Why this matters\n",
        "\n",
        "Website content changes and includes specific names, projects, and descriptions.  \n",
        "RAG makes answers more **document-based** and **verifiable**, especially for details that a model may otherwise guess.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-fUCj0KmJGX"
      },
      "source": [
        "## ðŸ”§ Step 1 â€” Install Python dependencies (run once)\n",
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## ðŸ¦™ Step 2 â€” Configure Ollama models (LLM + Embeddings)\n",
        "\n",
        "This step initializes two local Ollama components:\n",
        "\n",
        "- **LLM (generation model)**: used to produce answers.\n",
        "- **Embeddings model**: used to convert text chunks into vectors for retrieval (FAISS).\n",
        "\n",
        "**Inputs**\n",
        "- `LLM_MODEL = \"llama3.2\"`\n",
        "- `EMBED_MODEL = \"nomic-embed-text\"`\n",
        "\n",
        "**Outputs**\n",
        "- `llm`: a `ChatOllama` instance for text generation\n",
        "- `embeddings`: an `OllamaEmbeddings` instance for vector embeddings\n",
        "\n",
        "**Prerequisite**\n",
        "- The models must be available locally (pulled via Ollama), e.g.:\n",
        "  - `ollama pull llama3.2`\n",
        "  - `ollama pull nomic-embed-text`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "jmGu5Lr-mPZG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Ollama models configured: llama3.2 | nomic-embed-text\n"
          ]
        }
      ],
      "source": [
        "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "\n",
        "# --- Choose models you have pulled via `ollama pull ...`\n",
        "LLM_MODEL = \"llama3.2\"            # generation model\n",
        "EMBED_MODEL = \"nomic-embed-text\"  # embeddings model (recommended)\n",
        "\n",
        "# Initialize Ollama chat model\n",
        "llm = ChatOllama(model=LLM_MODEL, temperature=0.2)\n",
        "\n",
        "# Initialize Ollama embeddings model\n",
        "embeddings = OllamaEmbeddings(model=EMBED_MODEL)\n",
        "\n",
        "print(\"âœ… Ollama models configured:\", LLM_MODEL, \"|\", EMBED_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŒ Step 3 â€” Collect website content (crawl ~20 pages and save as `.txt`)\n",
        "\n",
        "This step builds a small local corpus from the **FAIR Data Innovations Hub** website so the RAG system can retrieve information from it.\n",
        "\n",
        "### Purpose\n",
        "\n",
        "- Visit a limited number of pages starting from the homepage (`https://fairdataihub.org/`)\n",
        "- Collect internal links using a simple breadth-first search (BFS)\n",
        "- Download each pageâ€™s HTML\n",
        "- Clean and extract readable text\n",
        "- Save each page as a separate `.txt` file in `data/fairdata_texts/`\n",
        "\n",
        "### Key settings\n",
        "\n",
        "- **START_URL**: the crawl entry point (`https://fairdataihub.org/`)\n",
        "- **MAX_PAGES = 20**: caps the number of pages to keep the dataset small and fast\n",
        "- **MAX_DEPTH = 2**: limits how far the crawler follows links from the start page\n",
        "- **OUT_DIR**: output folder for saved texts (`data/fairdata_texts/`)\n",
        "\n",
        "### How the crawler works\n",
        "\n",
        "1. **Initialize a web session**\n",
        "   - Uses `requests.Session()` with a browser-like `User-Agent` for stable page fetching.\n",
        "\n",
        "2. **Filter what to crawl**\n",
        "   - Skips authentication/legal and irrelevant pages using `SKIP_TERMS` (e.g., login, privacy, cookie).\n",
        "   - Skips non-text assets using `SKIP_EXT` (e.g., images, videos, zip files, PDFs).\n",
        "\n",
        "3. **Discover pages (BFS)**\n",
        "   - Uses a queue (`deque`) to explore internal links.\n",
        "   - Only keeps links that stay inside `fairdataihub.org`.\n",
        "   - Stops when it reaches `MAX_PAGES` or `MAX_DEPTH`.\n",
        "\n",
        "4. **Convert HTML â†’ clean text**\n",
        "   - Removes noisy elements such as `script`, `style`, `iframe`, and `form`.\n",
        "   - Removes common layout sections like `header`, `nav`, and `footer`.\n",
        "   - Extracts visible text and normalizes whitespace for readability.\n",
        "\n",
        "5. **Save outputs**\n",
        "   - Writes each page to a numbered `.txt` file (safe filename via `slugify`).\n",
        "   - Prepends the original page URL at the top of the text file for traceability.\n",
        "\n",
        "### Output\n",
        "\n",
        "After this step, you will have up to **20 text files** in:\n",
        "\n",
        "- `data/fairdata_texts/`\n",
        "\n",
        "These `.txt` files become the input documents for:\n",
        "- chunking (text splitting)\n",
        "- embedding (vectorization)\n",
        "- retrieval (FAISS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Collected 20 URLs\n",
            "[01] Saved -> 01_home.txt\n",
            "[02] Saved -> 02_projects.txt\n",
            "[03] Saved -> 03_team.txt\n",
            "[04] Saved -> 04_impact.txt\n",
            "[05] Saved -> 05_gallery.txt\n",
            "[06] Saved -> 06_blog.txt\n",
            "[07] Saved -> 07_events.txt\n",
            "[08] Saved -> 08_contact-us.txt\n",
            "[09] Saved -> 09_sodaforsparc.txt\n",
            "[10] Saved -> 10_aireadi.txt\n",
            "[11] Saved -> 11_posters-science.txt\n",
            "[12] Saved -> 12_eyeact.txt\n",
            "[13] Saved -> 13_careers.txt\n",
            "[14] Saved -> 14_codefair.txt\n",
            "[15] Saved -> 15_dmp-chef.txt\n",
            "[17] Saved -> 17_fair-biors.txt\n",
            "[18] Saved -> 18_fairshare.txt\n",
            "[19] Saved -> 19_knowmore.txt\n",
            "[20] Saved -> 20_sparclink.txt\n",
            "\n",
            "âœ… Done. Texts saved in: C:\\Users\\Nahid\\RAG_blog\\data\\fairdata_texts\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Crawl ~20 fairdataihub.org pages and save as .txt in data/fairdata_texts/\n",
        "\n",
        "!pip -q install requests beautifulsoup4\n",
        "\n",
        "import re\n",
        "from collections import deque\n",
        "from pathlib import Path\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "START_URL = \"https://fairdataihub.org/\"\n",
        "MAX_PAGES = 20\n",
        "MAX_DEPTH = 2\n",
        "\n",
        "OUT_DIR = Path(\"data/fairdata_texts\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "session = requests.Session()\n",
        "session.headers.update({\"User-Agent\": \"Mozilla/5.0 (fairdataihub-simple-rag)\"})\n",
        "\n",
        "SKIP_TERMS = [\"login\", \"signin\", \"signup\", \"register\", \"account\", \"privacy\", \"terms\", \"cookie\"]\n",
        "SKIP_EXT = (\".png\", \".jpg\", \".jpeg\", \".gif\", \".svg\", \".webp\", \".zip\", \".mp4\", \".pdf\")\n",
        "\n",
        "def normalize_url(u: str) -> str:\n",
        "    return u.split(\"#\", 1)[0].rstrip(\"/\")\n",
        "\n",
        "def is_fairdata_url(u: str) -> bool:\n",
        "    return urlparse(u).netloc.endswith(\"fairdataihub.org\")\n",
        "\n",
        "def should_skip(u: str) -> bool:\n",
        "    ul = u.lower()\n",
        "    return any(t in ul for t in SKIP_TERMS) or ul.endswith(SKIP_EXT)\n",
        "\n",
        "def slugify(u: str) -> str:\n",
        "    p = urlparse(u).path.strip(\"/\") or \"home\"\n",
        "    p = re.sub(r\"[^a-zA-Z0-9]+\", \"-\", p).strip(\"-\").lower()\n",
        "    return p[:120] or \"page\"\n",
        "\n",
        "def html_to_text(html: str) -> str:\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\", \"iframe\", \"form\"]):\n",
        "        tag.decompose()\n",
        "    for sel in [\"header\", \"footer\", \"nav\", \"aside\"]:\n",
        "        for t in soup.find_all(sel):\n",
        "            t.decompose()\n",
        "    text = soup.get_text(\"\\n\", strip=True)\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "    return text.strip()\n",
        "\n",
        "# 1) Collect internal URLs (BFS)\n",
        "visited = set()\n",
        "queue = deque([(START_URL, 0)])\n",
        "urls = []\n",
        "\n",
        "while queue and len(urls) < MAX_PAGES:\n",
        "    url, depth = queue.popleft()\n",
        "    url = normalize_url(url)\n",
        "\n",
        "    if url in visited or depth > MAX_DEPTH:\n",
        "        continue\n",
        "    if not is_fairdata_url(url) or should_skip(url):\n",
        "        continue\n",
        "\n",
        "    visited.add(url)\n",
        "    urls.append(url)\n",
        "\n",
        "    try:\n",
        "        r = session.get(url, timeout=20)\n",
        "        if r.status_code != 200:\n",
        "            continue\n",
        "        ctype = r.headers.get(\"content-type\", \"\").lower()\n",
        "        if \"text/html\" not in ctype:\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        for a in soup.find_all(\"a\", href=True):\n",
        "            nxt = normalize_url(urljoin(url, a[\"href\"]))\n",
        "            if is_fairdata_url(nxt) and not should_skip(nxt):\n",
        "                queue.append((nxt, depth + 1))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print(f\"âœ… Collected {len(urls)} URLs\")\n",
        "\n",
        "# 2) Save each page as .txt\n",
        "for i, url in enumerate(urls, start=1):\n",
        "    try:\n",
        "        r = session.get(url, timeout=20)\n",
        "        if r.status_code != 200:\n",
        "            continue\n",
        "        txt = html_to_text(r.text)\n",
        "        out = OUT_DIR / f\"{i:02d}_{slugify(url)}.txt\"\n",
        "        out.write_text(f\"URL: {url}\\n\\n{txt}\\n\", encoding=\"utf-8\")\n",
        "        print(f\"[{i:02d}] Saved -> {out.name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Failed {url}: {e}\")\n",
        "\n",
        "print(\"\\nâœ… Done. Texts saved in:\", OUT_DIR.resolve())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ‚ï¸ Step 4 â€” Load `.txt` pages and split into chunks\n",
        "\n",
        "This step prepares the website text for retrieval by converting raw page files into **manageable chunks**. Chunking is essential because retrieval works best when we search over smaller, semantically coherent pieces of text rather than entire pages.\n",
        "\n",
        "### Purpose\n",
        "\n",
        "- Load the saved website pages from `data/fairdata_texts/`\n",
        "- Convert each `.txt` file into a LangChain `Document`\n",
        "- Split documents into overlapping chunks suitable for embeddings + retrieval\n",
        "\n",
        "### What happens in this step\n",
        "\n",
        "1. **Read input files**\n",
        "   - Finds all `.txt` files inside `data/fairdata_texts/`\n",
        "   - If no files exist, it raises an error (meaning the crawl step didnâ€™t run or saved elsewhere)\n",
        "\n",
        "2. **Load as LangChain Documents**\n",
        "   - Each `.txt` file becomes one `Document`\n",
        "   - Adds a simple metadata field like `source_file` so we still know which page the text came from\n",
        "\n",
        "3. **Split into chunks**\n",
        "   - Uses a recursive text splitter to break long text into smaller pieces\n",
        "   - Chunks overlap slightly so important information isnâ€™t cut off between boundaries\n",
        "\n",
        "### Key settings (chunking)\n",
        "\n",
        "- **chunk_size = 1000 characters**  \n",
        "  Keeps chunks reasonably small for embedding and prompt context.\n",
        "\n",
        "- **chunk_overlap = 150 characters**  \n",
        "  Preserves continuity across chunk boundaries.\n",
        "\n",
        "- **separators = [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]**  \n",
        "  Prefers splitting at paragraph boundaries first, then lines, then spaces, and finally hard-splits as a last resort.\n",
        "\n",
        "### Output\n",
        "\n",
        "At the end of this step you have:\n",
        "\n",
        "- `docs`: a list of page-level Documents\n",
        "- `chunks`: a larger list of chunk-level Documents (the real unit used for embeddings + retrieval)\n",
        "\n",
        "These `chunks` will be embedded and indexed in the next step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Loaded 19 documents\n",
            "âœ… Total chunks: 120\n",
            "Sample chunk source: 01_home.txt\n",
            "Sample chunk text (first 250 chars):\n",
            "URL: https://fairdataihub.org\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Load saved .txt pages -> split into chunks\n",
        "\n",
        "from pathlib import Path\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "!pip -q install langchain langchain-community\n",
        "\n",
        "TXT_DIR = Path(\"data/fairdata_texts\")\n",
        "paths = sorted(TXT_DIR.glob(\"*.txt\"))\n",
        "\n",
        "if not paths:\n",
        "    raise FileNotFoundError(f\"No .txt files found in {TXT_DIR}. Run Step 1 first.\")\n",
        "\n",
        "# 1) Load documents\n",
        "docs = []\n",
        "for p in paths:\n",
        "    d = TextLoader(str(p), encoding=\"utf-8\").load()[0]\n",
        "    d.metadata[\"source_file\"] = p.name\n",
        "    docs.append(d)\n",
        "\n",
        "print(f\"âœ… Loaded {len(docs)} documents\")\n",
        "\n",
        "# 2) Split into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=150,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "print(\"âœ… Total chunks:\", len(chunks))\n",
        "print(\"Sample chunk source:\", chunks[0].metadata.get(\"source_file\"))\n",
        "print(\"Sample chunk text (first 250 chars):\")\n",
        "print(chunks[0].page_content[:250])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§  Step 5 â€” Embed chunks and build a FAISS vector index\n",
        "\n",
        "This step converts our text chunks into **vector embeddings** and stores them in a **FAISS** index. This index is what enables fast similarity search during RAG (i.e., retrieving the most relevant chunks for a user question).\n",
        "\n",
        "### Purpose\n",
        "\n",
        "- Connect to the local **Ollama** server\n",
        "- Use an **embedding model** to vectorize each chunk\n",
        "- Build a **FAISS vectorstore** for retrieval\n",
        "- (Optional) save the FAISS index to disk for reuse\n",
        "\n",
        "### What happens in this step\n",
        "\n",
        "1. **Install FAISS**\n",
        "   - Ensures FAISS is available to store and search embeddings efficiently.\n",
        "\n",
        "2. **Verify Ollama is running**\n",
        "   - Checks that the Ollama server is reachable at `http://localhost:11434`.\n",
        "   - If not reachable, retrieval indexing cannot proceed (because embeddings cannot be generated).\n",
        "\n",
        "3. **Create embeddings client**\n",
        "   - Initializes an embeddings interface using the selected Ollama embedding model.\n",
        "\n",
        "4. **Build the FAISS vectorstore**\n",
        "   - Embeds every chunk\n",
        "   - Inserts all vectors into a FAISS index\n",
        "   - Produces a `vectorstore` object that supports similarity search (retrieval)\n",
        "\n",
        "5. **Save the index (optional)**\n",
        "   - Writes the FAISS index to a folder (e.g., `faiss_index_fairdata/`)\n",
        "   - Useful to avoid rebuilding embeddings each time you restart the notebook\n",
        "\n",
        "### Key settings\n",
        "\n",
        "- **BASE_URL**: location of the Ollama server (default: `http://localhost:11434`)\n",
        "- **EMBED_MODEL**: embedding model name (e.g., `nomic-embed-text` or `mxbai-embed-large`)\n",
        "- **INDEX_DIR**: folder name used to persist the FAISS index\n",
        "\n",
        "### Output\n",
        "\n",
        "At the end of this step you have:\n",
        "\n",
        "- `vectorstore`: a FAISS-backed index containing all embedded chunks  \n",
        "- A saved index folder (optional): `faiss_index_fairdata/`\n",
        "\n",
        "This `vectorstore` is used in the next step to retrieve relevant context for RAG answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Ollama reachable.\n",
            "âœ… FAISS index built and saved to ./faiss_index_fairdata\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Build FAISS vectorstore from chunks using Ollama embeddings\n",
        "\n",
        "!pip -q install faiss-cpu\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "import requests\n",
        "\n",
        "BASE_URL = \"http://localhost:11434\"\n",
        "EMBED_MODEL = \"nomic-embed-text\"  # or \"mxbai-embed-large\"\n",
        "\n",
        "# Optional: check Ollama is reachable\n",
        "try:\n",
        "    r = requests.get(f\"{BASE_URL}/api/tags\", timeout=5)\n",
        "    print(\"âœ… Ollama reachable.\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\n",
        "        f\"Cannot reach Ollama at {BASE_URL}. \"\n",
        "        \"Make sure Ollama is running (ollama serve). \"\n",
        "        f\"Error: {e}\"\n",
        "    )\n",
        "\n",
        "embeddings = OllamaEmbeddings(model=EMBED_MODEL, base_url=BASE_URL)\n",
        "\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "# Optional: save locally\n",
        "INDEX_DIR = \"faiss_index_fairdata\"\n",
        "vectorstore.save_local(INDEX_DIR)\n",
        "\n",
        "print(f\"âœ… FAISS index built and saved to ./{INDEX_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ… Step 6 â€” Compare No-RAG vs RAG question answering (no citations)\n",
        "\n",
        "This step demonstrates the core idea of the project: answering the **same questions** with and without retrieval.\n",
        "\n",
        "### Purpose\n",
        "\n",
        "- **No-RAG mode:** the LLM answers using only its general knowledge (higher risk of missing details or hallucination).\n",
        "- **RAG mode:** the system retrieves relevant chunks from our FAISS index and forces the LLM to answer using only that website-derived context.\n",
        "\n",
        "### What happens in this step\n",
        "\n",
        "1. **Initialize the local LLM**\n",
        "   - Connects to the Ollama server and loads a chat model (e.g., `llama3.2`).\n",
        "   - Uses a low temperature for more consistent answers.\n",
        "\n",
        "2. **Define the No-RAG function**\n",
        "   - Sends the question directly to the LLM.\n",
        "   - Includes a safety instruction: if unsure, the model should say so instead of inventing facts.\n",
        "\n",
        "3. **Define the RAG function**\n",
        "   - Converts the FAISS `vectorstore` into a retriever (`k=5`).\n",
        "   - Retrieves the top `k` most relevant chunks for the question.\n",
        "   - Concatenates retrieved chunks into a short context block.\n",
        "   - Prompts the LLM to answer using **only** the provided context.\n",
        "   - If the answer is missing from the context, it must respond with:  \n",
        "     â€œNot found in the provided pages.â€\n",
        "\n",
        "4. **Run a small evaluation set**\n",
        "   - Executes a list of website-related questions.\n",
        "   - Prints **No-RAG** and **RAG** answers side by side to show the difference.\n",
        "\n",
        "### Why this demonstrates the power of RAG\n",
        "\n",
        "RAG is strongest when questions require **site-specific facts**, such as:\n",
        "- leadership or team member names,\n",
        "- project titles and descriptions,\n",
        "- tool names and their functions.\n",
        "\n",
        "Without retrieval, the model may guess or omit details. With retrieval, the answer is grounded in the downloaded website text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==============================================================================================================\n",
            "QUESTION: What is FAIR Data Innovations Hub and what does it do?\n",
            "\n",
            "--- NO RAG ---\n",
            "I am not sure.\n",
            "\n",
            "--- WITH RAG ---\n",
            "The FAIR Data Innovations Hub is a platform that provides tools and resources to make FAIR (Findable, Accessible, Interoperable, Reusable) data sharing effortless. It includes a web platform with intuitive user interfaces and automation tools to help researchers manage, curate, and share FAIR, ethically-sourced, and AI-ready datasets.\n",
            "\n",
            "==============================================================================================================\n",
            "QUESTION: Who are the members of the FAIR Data Innovations Hub team?\n",
            "\n",
            "--- NO RAG ---\n",
            "I am not sure who the specific members of the FAIR Data Innovations Hub team are.\n",
            "\n",
            "--- WITH RAG ---\n",
            "Bhavesh Patel\n",
            "Dorian Portillo\n",
            "Sanjay Soundarajan\n",
            "\n",
            "==============================================================================================================\n",
            "QUESTION: What is DMP chef and what does it do?\n",
            "\n",
            "--- NO RAG ---\n",
            "I am not sure.\n",
            "\n",
            "--- WITH RAG ---\n",
            "DMP Chef is a web tool that helps researchers create compliant and machine-actionable Data Management Plans (DMPs) in minutes. It generates drafts of funder-compliant DMPs tailored to their grant proposals, even creating machine-actionable versions.\n",
            "\n",
            "==============================================================================================================\n",
            "QUESTION: List the main projects mentioned on the FAIR Data Innovations Hub website.\n",
            "\n",
            "--- NO RAG ---\n",
            "I am not sure.\n",
            "\n",
            "--- WITH RAG ---\n",
            "The main projects mentioned on the FAIR Data Innovations Hub website are:\n",
            "\n",
            "1. SODA (Streamlined Organization of Data for the SPARC Portal)\n",
            "2. AI-READI (AI-Ready and Exploratory Atlas for Diabetes Insights)\n",
            "3. Eye ACT\n",
            "4. DMP Chef\n",
            "5. Codefair\n"
          ]
        }
      ],
      "source": [
        "# Step 6: NO-RAG vs RAG answering \n",
        "\n",
        "try:\n",
        "    from langchain_ollama import ChatOllama\n",
        "except Exception:\n",
        "    from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "BASE_URL = \"http://localhost:11434\"\n",
        "LLM_MODEL = \"llama3.2\"\n",
        "\n",
        "llm = ChatOllama(model=LLM_MODEL, base_url=BASE_URL, temperature=0.2)\n",
        "\n",
        "def answer_without_rag(question: str) -> str:\n",
        "    prompt = f\"\"\"Answer the question as best you can.\n",
        "\n",
        "If you are not sure, say \"I am not sure\" and do NOT invent details.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    return llm.invoke(prompt).content\n",
        "\n",
        "def answer_with_rag(question: str, k: int = 5) -> str:\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
        "    retrieved = retriever.invoke(question)\n",
        "\n",
        "    # keep context short to avoid overload\n",
        "    context = \"\\n\\n---\\n\\n\".join(d.page_content for d in retrieved)\n",
        "\n",
        "    prompt = f\"\"\"Answer the question using ONLY the website context below.\n",
        "If the answer is not in the context, say: \"Not found in the provided pages.\"\n",
        "\n",
        "Website context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    return llm.invoke(prompt).content\n",
        "\n",
        "\n",
        "questions = [\n",
        "    \"What is FAIR Data Innovations Hub and what does it do?\",\n",
        "    \"Who are the members of the FAIR Data Innovations Hub team?\",\n",
        "    \"What is DMP chef and what does it do?\",\n",
        "    \"List the main projects mentioned on the FAIR Data Innovations Hub website.\",\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    print(\"\\n\" + \"=\"*110)\n",
        "    print(\"QUESTION:\", q)\n",
        "\n",
        "    print(\"\\n--- NO RAG ---\")\n",
        "    print(answer_without_rag(q))\n",
        "\n",
        "    print(\"\\n--- WITH RAG ---\")\n",
        "    print(answer_with_rag(q, k=5))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv (3.10.11)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
